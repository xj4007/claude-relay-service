const crypto = require('crypto')
const redis = require('../models/redis')
const logger = require('../utils/logger')
const requestQueue = require('../utils/requestQueue')

/**
 * å“åº”ç¼“å­˜æœåŠ¡
 * ç”¨äºç¼“å­˜å®¢æˆ·ç«¯æ–­å¼€ä½†ä¸Šæ¸¸æˆåŠŸè¿”å›çš„å“åº”
 * é¿å…å®¢æˆ·ç«¯é‡è¯•æ—¶é‡å¤è¯·æ±‚ä¸Šæ¸¸
 *
 * æ–°åŠŸèƒ½ï¼š
 * - è¯·æ±‚å»é‡å’Œç­‰å¾…å…±äº«ï¼ˆå¤šä¸ªç›¸åŒè¯·æ±‚å…±äº«ä¸€ä¸ªä¸Šæ¸¸è°ƒç”¨ï¼‰
 * - å¢åŠ TTLåˆ°5åˆ†é’Ÿ
 */
class ResponseCacheService {
  constructor() {
    this.CACHE_PREFIX = 'response_cache:'
    this.STREAM_CACHE_PREFIX = 'stream_cache:'
    this.DEFAULT_TTL = 300 // 5åˆ†é’Ÿï¼ˆä»180ç§’æ”¹ä¸º300ç§’ï¼‰
    this.MAX_CACHE_SIZE = 5 * 1024 * 1024 // 5MB
  }

  /**
   * ç”Ÿæˆç¼“å­˜é”®ï¼ˆåŸºäºè¯·æ±‚å†…å®¹çš„å”¯ä¸€å“ˆå¸Œï¼‰
   * @param {Object} requestBody - è¯·æ±‚ä½“
   * @param {string} model - æ¨¡å‹åç§°
   * @returns {string} - ç¼“å­˜é”®
   */
  generateCacheKey(requestBody, model) {
    try {
      // æ„å»ºç¼“å­˜é”®çš„å†…å®¹ï¼ˆåŒ…å«æ‰€æœ‰å½±å“è¾“å‡ºçš„å‚æ•°ï¼‰
      // âš ï¸ å¿…é¡»æŒ‰å›ºå®šé¡ºåºæ„å»ºï¼Œç¡®ä¿ç›¸åŒå†…å®¹ç”Ÿæˆç›¸åŒå“ˆå¸Œ
      const cacheContent = {
        model: model,
        messages: requestBody.messages || [],
        system: requestBody.system || '',
        max_tokens: requestBody.max_tokens,
        temperature: requestBody.temperature,
        top_p: requestBody.top_p,
        top_k: requestBody.top_k,
        stop_sequences: requestBody.stop_sequences
        // ä¸åŒ…å« metadata å’Œ streamï¼Œå› ä¸ºè¿™äº›ä¸å½±å“è¾“å‡ºå†…å®¹
      }

      // ğŸ”§ ä½¿ç”¨ç¨³å®šçš„åºåˆ—åŒ–æ–¹å¼ï¼ˆæŒ‰é”®æ’åºï¼‰
      const stableJson = JSON.stringify(cacheContent, Object.keys(cacheContent).sort())

      // ç”Ÿæˆ SHA256 å“ˆå¸Œ
      const hash = crypto.createHash('sha256').update(stableJson).digest('hex').substring(0, 32)

      logger.debug(`ğŸ“‹ Cache key generated: ${hash}`)
      return hash
    } catch (error) {
      logger.error(`âŒ Failed to generate cache key: ${error.message}`)
      return null
    }
  }

  /**
   * æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ï¼ˆéæµå¼ï¼‰
   * @param {string} cacheKey - ç¼“å­˜é”®
   * @returns {Object|null} - ç¼“å­˜çš„å“åº”ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› null
   */
  async getCachedResponse(cacheKey) {
    if (!cacheKey) return null

    try {
      const client = redis.getClientSafe()
      const redisKey = `${this.CACHE_PREFIX}${cacheKey}`
      const cached = await client.hgetall(redisKey)

      if (!cached || !cached.body) {
        logger.debug(`ğŸ“‹ Cache miss: ${cacheKey}`)
        return null
      }

      // è§£æç¼“å­˜çš„å“åº”
      const response = {
        statusCode: parseInt(cached.statusCode) || 200,
        headers: JSON.parse(cached.headers || '{}'),
        body: JSON.parse(cached.body),
        usage: cached.usage ? JSON.parse(cached.usage) : null,
        cachedAt: parseInt(cached.cachedAt) || Date.now()
      }

      logger.info(
        `ğŸ¯ Cache hit: ${cacheKey} | Cached ${Math.floor((Date.now() - response.cachedAt) / 1000)}s ago`
      )
      return response
    } catch (error) {
      logger.error(`âŒ Failed to get cached response: ${error.message}`)
      return null
    }
  }

  /**
   * ğŸ†• è·å–ç¼“å­˜æˆ–æ‰§è¡Œè¯·æ±‚ï¼ˆå¸¦è¯·æ±‚å»é‡å’Œç­‰å¾…å…±äº«ï¼‰
   * å¦‚æœç¼“å­˜å­˜åœ¨ï¼Œç›´æ¥è¿”å›ç¼“å­˜
   * å¦‚æœæ­£åœ¨è¯·æ±‚ä¸­ï¼Œç­‰å¾…å¹¶å…±äº«ç»“æœ
   * å¦‚æœéƒ½æ²¡æœ‰ï¼Œæ‰§è¡Œæ–°è¯·æ±‚å¹¶ç¼“å­˜
   *
   * @param {string} cacheKey - ç¼“å­˜é”®
   * @param {Function} fetchFn - è¯·æ±‚å‡½æ•° async () => response
   * @param {number} ttl - ç¼“å­˜TTLï¼ˆç§’ï¼‰
   * @returns {Promise<Object>} - å“åº”å¯¹è±¡
   */
  async getOrFetchResponse(cacheKey, fetchFn, ttl = this.DEFAULT_TTL) {
    if (!cacheKey) {
      // æ²¡æœ‰ç¼“å­˜é”®ï¼Œç›´æ¥æ‰§è¡Œè¯·æ±‚
      return await fetchFn()
    }

    // 1. å…ˆæ£€æŸ¥ç¼“å­˜
    const cached = await this.getCachedResponse(cacheKey)
    if (cached) {
      logger.info(`âœ… Returning cached response | CacheKey: ${cacheKey.substring(0, 16)}...`)
      return cached
    }

    // 2. æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒè¯·æ±‚æ­£åœ¨è¿›è¡Œï¼Œå¦‚æœæœ‰åˆ™ç­‰å¾…
    // å¦‚æœæ²¡æœ‰ï¼Œåˆ™æ‰§è¡Œæ–°è¯·æ±‚
    const result = await requestQueue.executeOrWait(cacheKey, async () => {
      logger.info(`ğŸš€ Executing new upstream request | CacheKey: ${cacheKey.substring(0, 16)}...`)

      // æ‰§è¡Œå®é™…è¯·æ±‚
      const response = await fetchFn()

      // ğŸ” æ£€æµ‹ç‰¹æ®Šé”™è¯¯å“åº”ï¼ˆç©ºå“åº”ä½“ã€JSONè§£æå¤±è´¥ç­‰ï¼‰
      const shouldRetryDueToSpecialError = this._shouldRetryForSpecialError(response)
      if (shouldRetryDueToSpecialError) {
        logger.warn(
          `ğŸ”„ Detected special error response: ${shouldRetryDueToSpecialError} | CacheKey: ${cacheKey.substring(0, 16)}...`
        )
        // æ ‡è®°ä¸ºå¤±è´¥ï¼Œè®©ç­‰å¾…çš„è¯·æ±‚é‡æ–°å°è¯•è€Œä¸æ˜¯å…±äº«è¿™ä¸ªæœ‰é—®é¢˜çš„å“åº”
        return { success: false, response }
      }

      // ç¼“å­˜æˆåŠŸçš„å“åº”ï¼ˆåªç¼“å­˜2xxå“åº”ï¼‰
      if (response.statusCode >= 200 && response.statusCode < 300) {
        await this.cacheResponse(cacheKey, response, ttl)
        // æ ‡è®°ä¸ºæˆåŠŸï¼Œè®©ç­‰å¾…çš„è¯·æ±‚å…±äº«æ­¤ç»“æœ
        return { success: true, response }
      } else {
        logger.debug(
          `âš ï¸ Not caching non-2xx response: ${response.statusCode} | CacheKey: ${cacheKey.substring(0, 16)}...`
        )
        // æ ‡è®°ä¸ºå¤±è´¥ï¼Œè®©ç­‰å¾…çš„è¯·æ±‚é‡æ–°å°è¯•
        return { success: false, response }
      }
    })

    // 3. å¦‚æœæ˜¯å¤±è´¥å“åº”ï¼Œç­‰å¾…çš„è¯·æ±‚åº”è¯¥é‡æ–°å°è¯•è€Œä¸æ˜¯å…±äº«å¤±è´¥ç»“æœ
    if (!result.success) {
      logger.warn(
        `âš ï¸ Shared request failed (${result.response.statusCode}), waiting request will retry independently | CacheKey: ${cacheKey.substring(0, 16)}...`
      )
      // ğŸ”„ é‡æ–°æ‰§è¡Œè¯·æ±‚ï¼ˆå¸¦é‡è¯•é€»è¾‘ï¼‰ï¼Œä¸å…±äº«å¤±è´¥ç»“æœ
      return await fetchFn()
    }

    return result.response
  }

  /**
   * ç¼“å­˜å“åº”ï¼ˆéæµå¼ï¼‰
   * @param {string} cacheKey - ç¼“å­˜é”®
   * @param {Object} response - å“åº”å¯¹è±¡
   * @param {number} ttl - è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
   */
  async cacheResponse(cacheKey, response, ttl = this.DEFAULT_TTL) {
    if (!cacheKey) return

    try {
      const client = redis.getClientSafe()
      const redisKey = `${this.CACHE_PREFIX}${cacheKey}`

      // æ£€æŸ¥å“åº”å¤§å°
      const bodySize = JSON.stringify(response.body).length
      if (bodySize > this.MAX_CACHE_SIZE) {
        logger.warn(
          `âš ï¸ Response too large to cache: ${(bodySize / 1024 / 1024).toFixed(2)}MB > ${this.MAX_CACHE_SIZE / 1024 / 1024}MB`
        )
        return
      }

      // å­˜å‚¨åˆ° Redis Hash
      const cacheData = {
        statusCode: response.statusCode.toString(),
        headers: JSON.stringify(response.headers),
        body: JSON.stringify(response.body),
        usage: response.usage ? JSON.stringify(response.usage) : '',
        cachedAt: Date.now().toString()
      }

      await client.hset(redisKey, cacheData)
      await client.expire(redisKey, ttl)

      logger.info(
        `ğŸ’¾ Cached response: ${cacheKey} | Size: ${(bodySize / 1024).toFixed(2)}KB | TTL: ${ttl}s`
      )
    } catch (error) {
      logger.error(`âŒ Failed to cache response: ${error.message}`)
    }
  }

  /**
   * æ£€æŸ¥æµå¼ç¼“å­˜æ˜¯å¦å­˜åœ¨
   * @param {string} cacheKey - ç¼“å­˜é”®
   * @returns {Array|null} - ç¼“å­˜çš„ chunks æ•°ç»„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› null
   */
  async getCachedStream(cacheKey) {
    if (!cacheKey) return null

    try {
      const client = redis.getClientSafe()
      const redisKey = `${this.STREAM_CACHE_PREFIX}${cacheKey}`

      // æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´
      const metadata = await client.hgetall(`${redisKey}:meta`)
      if (!metadata || metadata.complete !== 'true') {
        logger.debug(`ğŸ“‹ Stream cache miss or incomplete: ${cacheKey}`)
        return null
      }

      // è·å–æ‰€æœ‰ chunks
      const chunks = await client.lrange(redisKey, 0, -1)
      if (!chunks || chunks.length === 0) {
        return null
      }

      logger.info(
        `ğŸ¯ Stream cache hit: ${cacheKey} | ${chunks.length} chunks | Cached ${Math.floor((Date.now() - parseInt(metadata.cachedAt)) / 1000)}s ago`
      )
      return chunks.map((chunk) => JSON.parse(chunk))
    } catch (error) {
      logger.error(`âŒ Failed to get cached stream: ${error.message}`)
      return null
    }
  }

  /**
   * å¼€å§‹ç¼“å­˜æµå¼å“åº”
   * @param {string} cacheKey - ç¼“å­˜é”®
   * @returns {Object} - ç¼“å­˜æ”¶é›†å™¨å¯¹è±¡
   */
  createStreamCacheCollector(cacheKey) {
    if (!cacheKey) return null

    const chunks = []
    let totalSize = 0
    let isComplete = false

    return {
      /**
       * æ·»åŠ ä¸€ä¸ª chunk
       * @param {Object} chunk - SSE chunk å¯¹è±¡
       */
      addChunk(chunk) {
        const chunkStr = JSON.stringify(chunk)
        const chunkSize = chunkStr.length

        // æ£€æŸ¥å¤§å°é™åˆ¶
        if (totalSize + chunkSize > this.MAX_CACHE_SIZE) {
          logger.warn(`âš ï¸ Stream cache size limit reached, stopping collection`)
          return false
        }

        chunks.push(chunk)
        totalSize += chunkSize

        // æ£€æŸ¥æ˜¯å¦å®Œæˆ
        if (chunk.event === 'message_stop') {
          isComplete = true
        }

        return true
      },

      /**
       * ä¿å­˜åˆ° Redisï¼ˆåªæœ‰å®Œæ•´æ¥æ”¶æ‰ä¿å­˜ï¼‰
       * @param {number} ttl - è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
       */
      async save(ttl = this.DEFAULT_TTL) {
        if (!isComplete) {
          logger.debug(`ğŸ“‹ Stream incomplete, not caching: ${cacheKey}`)
          return
        }

        try {
          const client = redis.getClientSafe()
          const redisKey = `${this.STREAM_CACHE_PREFIX}${cacheKey}`

          // æ¸…ç©ºæ—§æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
          await client.del(redisKey)
          await client.del(`${redisKey}:meta`)

          // å­˜å‚¨æ‰€æœ‰ chunks
          for (const chunk of chunks) {
            await client.rpush(redisKey, JSON.stringify(chunk))
          }

          // å­˜å‚¨å…ƒæ•°æ®
          await client.hset(`${redisKey}:meta`, {
            complete: 'true',
            cachedAt: Date.now().toString(),
            chunkCount: chunks.length.toString()
          })

          // è®¾ç½®è¿‡æœŸæ—¶é—´
          await client.expire(redisKey, ttl)
          await client.expire(`${redisKey}:meta`, ttl)

          logger.info(
            `ğŸ’¾ Cached stream: ${cacheKey} | ${chunks.length} chunks | Size: ${(totalSize / 1024).toFixed(2)}KB | TTL: ${ttl}s`
          )
        } catch (error) {
          logger.error(`âŒ Failed to save stream cache: ${error.message}`)
        }
      },

      /**
       * è·å–æ”¶é›†çŠ¶æ€
       */
      getStats() {
        return {
          chunkCount: chunks.length,
          totalSize,
          isComplete
        }
      }
    }
  }

  /**
   * æ¸…é™¤æŒ‡å®šçš„ç¼“å­˜
   * @param {string} cacheKey - ç¼“å­˜é”®
   */
  async clearCache(cacheKey) {
    if (!cacheKey) return

    try {
      const client = redis.getClientSafe()
      await client.del(`${this.CACHE_PREFIX}${cacheKey}`)
      await client.del(`${this.STREAM_CACHE_PREFIX}${cacheKey}`)
      await client.del(`${this.STREAM_CACHE_PREFIX}${cacheKey}:meta`)
      logger.debug(`ğŸ—‘ï¸ Cleared cache: ${cacheKey}`)
    } catch (error) {
      logger.error(`âŒ Failed to clear cache: ${error.message}`)
    }
  }

  /**
   * è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
   */
  async getStats() {
    try {
      const client = redis.getClientSafe()

      // ç»Ÿè®¡éæµå¼ç¼“å­˜
      const responseCacheKeys = await client.keys(`${this.CACHE_PREFIX}*`)
      let totalResponseSize = 0
      for (const key of responseCacheKeys) {
        const body = await client.hget(key, 'body')
        if (body) totalResponseSize += body.length
      }

      // ç»Ÿè®¡æµå¼ç¼“å­˜
      const streamCacheKeys = await client.keys(`${this.STREAM_CACHE_PREFIX}*`)
      const streamCacheCount = streamCacheKeys.filter((k) => !k.endsWith(':meta')).length

      return {
        responseCacheCount: responseCacheKeys.length,
        responseCacheSizeMB: (totalResponseSize / 1024 / 1024).toFixed(2),
        streamCacheCount,
        ttlSeconds: this.DEFAULT_TTL,
        maxCacheSizeMB: this.MAX_CACHE_SIZE / 1024 / 1024
      }
    } catch (error) {
      logger.error(`âŒ Failed to get cache stats: ${error.message}`)
      return null
    }
  }

  /**
   * ğŸ” æ£€æµ‹æ˜¯å¦ä¸ºéœ€è¦é‡è¯•çš„ç‰¹æ®Šé”™è¯¯å“åº”
   * @param {Object} response - å“åº”å¯¹è±¡
   * @returns {string|null} - éœ€è¦é‡è¯•æ—¶è¿”å›åŸå› æè¿°ï¼Œå¦åˆ™è¿”å› null
   */
  _shouldRetryForSpecialError(response) {
    if (!response || !response.statusCode) {
      return 'missing response or status code'
    }

    const { statusCode } = response
    let bodyText = ''

    // è·å–å“åº”ä½“æ–‡æœ¬
    if (typeof response.body === 'string') {
      bodyText = response.body
    } else if (response.body !== undefined && response.body !== null) {
      try {
        bodyText = JSON.stringify(response.body)
      } catch (error) {
        return 'failed to stringify response body'
      }
    }

    const normalizedText = bodyText.toLowerCase()

    // ğŸ†• æ£€æµ‹ç©ºå“åº”ä½“æˆ–æ— æ•ˆ JSONï¼ˆçŠ¶æ€ç  200 ä½†å“åº”ä½“å¼‚å¸¸ï¼‰
    if (statusCode === 200 || statusCode === 201) {
      // æ£€æµ‹å®Œå…¨ç©ºçš„å“åº”ä½“
      if (!bodyText || bodyText.trim() === '') {
        return 'empty response body with 200 status'
      }

      // æ£€æµ‹å“åº”ä½“è¿‡çŸ­ï¼ˆå¯èƒ½æ˜¯æˆªæ–­çš„å“åº”ï¼‰
      if (bodyText.length < 10 && !bodyText.includes('{')) {
        return 'suspiciously short response body'
      }

      // å°è¯•è§£æ JSONï¼Œå¦‚æœå¤±è´¥è¯´æ˜æ ¼å¼æœ‰é—®é¢˜
      try {
        const parsed = JSON.parse(bodyText)
        // æ£€æµ‹ç¼ºå°‘å¿…è¦å­—æ®µçš„å“åº”ï¼ˆClaude API åº”è¯¥åŒ…å«è¿™äº›å­—æ®µï¼‰
        if (parsed && typeof parsed === 'object') {
          const hasValidStructure =
            parsed.content ||
            parsed.message ||
            parsed.error ||
            parsed.type ||
            (Array.isArray(parsed.content) && parsed.content.length > 0)

          if (!hasValidStructure) {
            return 'invalid claude api response structure'
          }
        }
      } catch (jsonError) {
        return 'malformed json response with 200 status'
      }
    }

    // æ£€æµ‹å…¶ä»–ç‰¹æ®Šé”™è¯¯
    if (statusCode === 400) {
      const thinkingMismatch =
        normalizedText.includes('expected `thinking`') &&
        normalizedText.includes('found `tool_use`')

      if (thinkingMismatch) {
        return 'thinking/tool_use format mismatch'
      }

      const officialInternalError =
        normalizedText.includes('"type":"internal_error"') ||
        normalizedText.includes("'type':'internal_error'") ||
        normalizedText.includes('server internal error, please contact admin')

      if (officialInternalError) {
        return 'official internal error 400'
      }

      // ğŸ†• æ£€æµ‹ thinking.budget_tokens ç›¸å…³é”™è¯¯
      const thinkingBudgetError =
        normalizedText.includes('max_tokens') && normalizedText.includes('thinking.budget_tokens')

      if (thinkingBudgetError) {
        return 'thinking budget tokens validation error'
      }
    }

    if (statusCode === 524) {
      return 'cloudflare timeout 524'
    }

    return null
  }
}

module.exports = new ResponseCacheService()
